# -*- coding: utf-8 -*-
"""MCAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uIZWJaYwD1oGYsECKswu5D6IHJ5BRnDn

# Setup and Data **Loading**
"""

# If running in Google Colab, mount drive to access the dataset in Drive
try:
    from google.colab import drive
    drive.mount('/content/drive')
except:
    print("Drive mount not required (not running in Colab environment).")

# Install FAISS for similarity search
!pip install faiss-cpu

import os, random, math
import numpy as np
import pandas as pd
import torch
import torchaudio
from torch import nn, optim
import torch.nn.functional as F
import librosa
import matplotlib.pyplot as plt

# Check device (GPU or CPU)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Using device:", device)

# Set random seeds for reproducibility
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
if device == 'cuda':
    torch.cuda.manual_seed_all(seed)

# Define base directories for the FMA dataset on Drive
DRIVE_BASE = '/content/drive/MyDrive'  # adjust if your Drive base path is different
FMA_SMALL_DIR = f'{DRIVE_BASE}/fma_small'
FMA_META_DIR  = f'{DRIVE_BASE}/fma_metadata'

# Load the track metadata CSV
tracks_csv = pd.read_csv(f'{FMA_META_DIR}/tracks.csv', header=[0,1], index_col=0)
# Filter to the "small" subset and the splits
subset_small = tracks_csv[tracks_csv[('set', 'subset')] == 'small']
train_df = subset_small[subset_small[('set', 'split')] == 'training']
val_df   = subset_small[subset_small[('set', 'split')] == 'validation']
test_df  = subset_small[subset_small[('set', 'split')] == 'test']
print(f"FMA-small dataset: total={len(subset_small)}, train={len(train_df)}, val={len(val_df)}, test={len(test_df)}")

# Helper to get file path from track ID
def get_audio_path(track_id, base_dir=FMA_SMALL_DIR):
    tid_str = f"{track_id:06d}"
    return os.path.join(base_dir, tid_str[:3], f"{tid_str}.mp3")

# Build lists of file paths for each split
train_files = [get_audio_path(tid) for tid in train_df.index]
val_files   = [get_audio_path(tid) for tid in val_df.index]
test_files  = [get_audio_path(tid) for tid in test_df.index]

# Quick sanity-check: print an example path and verify file exists
print("Example train file:", train_files[0], "exists?", os.path.exists(train_files[0]))

"""# Data Preparation and Augmentation"""

# Audio augmentation class: adds noise and impulse responses
class AudioAugmentor:
    """Apply waveform augmentations: time offset, noise, and impulse response filtering."""
    def __init__(self, noise_files=None):
        # Pre-load noise clips if provided (to mix in background); if none, we'll use white noise
        self.noise_clips = []
        if noise_files:
            for nf in noise_files:
                try:
                    wav, sr = torchaudio.load(nf)
                    wav = torchaudio.transforms.Resample(sr, 8000)(wav)  # resample noise to 8kHz
                    self.noise_clips.append(wav.squeeze(0))
                except Exception as e:
                    continue
        if len(self.noise_clips) == 0:
            self.noise_clips = None  # use white noise if no noise clips available

    def add_background_noise(self, waveform, snr_db):
        """Add background noise to waveform at the given SNR (dB)."""
        if waveform.abs().max() < 1e-6:
            # If silence segment, just add very low-level noise
            return waveform + torch.randn_like(waveform) * 1e-6
        # Compute signal power and target noise power
        sig_power = (waveform**2).mean()
        noise_power = sig_power / (10**(snr_db/10.0))
        noise_std = noise_power.sqrt()
        # Get a noise waveform of same length
        if self.noise_clips:
            # pick a random pre-loaded noise clip and segment of equal length
            noise = random.choice(self.noise_clips)
            if len(noise) >= len(waveform):
                start = random.randint(0, len(noise) - len(waveform))
                noise_segment = noise[start:start+len(waveform)]
            else:
                # if noise clip is shorter, repeat it to cover length
                repeats = math.ceil(len(waveform) / len(noise))
                noise_segment = noise.repeat(repeats)[:len(waveform)]
            noise_segment = noise_segment.to(waveform.device)
        else:
            # generate white noise
            noise_segment = torch.randn_like(waveform)
        # Scale noise to desired power
        noise_segment = noise_std * noise_segment / (noise_segment.std() + 1e-10)
        return waveform + noise_segment

    def apply_impulse_response(self, waveform, sample_rate=8000):
        """Apply a synthetic room impulse response: two echo delays and a random lowpass or highpass filter."""
        # Simulate two decaying echoes
        delay1 = int(0.05 * sample_rate)  # ~50 ms delay
        delay2 = int(0.1 * sample_rate)   # ~100 ms delay
        amp1 = random.uniform(0.3, 0.7)   # echo amplitude factors
        amp2 = random.uniform(0.1, 0.5)
        # Add echoes (if waveform long enough for delay)
        if len(waveform) > delay1:
            waveform = waveform + amp1 * F.pad(waveform[:-delay1], (delay1, 0))
        if len(waveform) > delay2:
            waveform = waveform + amp2 * F.pad(waveform[:-delay2], (delay2, 0))
        # Randomly apply either lowpass or highpass filter to simulate frequency response
        if random.random() < 0.5:
            cutoff = random.uniform(2000, 4000)  # lowpass cutoff 2-4 kHz
            waveform = torchaudio.functional.lowpass_biquad(waveform.unsqueeze(0), sample_rate, cutoff).squeeze(0)
        else:
            cutoff = random.uniform(100, 400)    # highpass cutoff 100-400 Hz
            waveform = torchaudio.functional.highpass_biquad(waveform.unsqueeze(0), sample_rate, cutoff).squeeze(0)
        return waveform

    def process(self, audio, orig_start, aug_start):
        """
        Given a raw audio Tensor (full track) and two start times (seconds),
        return a 1s original segment and an augmented 1s segment.
        """
        sr_orig = orig_audio_sr  # use global original sample rate (set before call)
        # Convert start times (in seconds) to sample indices
        orig_idx = int(orig_start * sr_orig)
        aug_idx  = int(aug_start * sr_orig)
        orig_seg = audio[orig_idx : orig_idx + int(sr_orig * 1.0)]
        aug_seg  = audio[aug_idx  : aug_idx  + int(sr_orig * 1.0)]
        # Pad segments with zeros if they are shorter than 1s (e.g., near end of track)
        if len(orig_seg) < sr_orig:
            orig_seg = F.pad(orig_seg, (0, int(sr_orig) - len(orig_seg)))
        if len(aug_seg) < sr_orig:
            aug_seg = F.pad(aug_seg, (0, int(sr_orig) - len(aug_seg)))
        # Resample both segments to 8000 Hz (model input requirement)
        if sr_orig != 8000:
            orig_seg = torchaudio.functional.resample(orig_seg, sr_orig, 8000)
            aug_seg  = torchaudio.functional.resample(aug_seg,  sr_orig, 8000)
        # Apply augmentations to the augmented segment:
        # Add background noise
        snr = random.uniform(0, 10)  # random SNR between 0 and 10 dB
        aug_seg = self.add_background_noise(aug_seg, snr)
        # Apply impulse response filtering (echo + filter)
        aug_seg = self.apply_impulse_response(aug_seg, sample_rate=8000)
        # Ensure both segments have the same length after augmentation (trim any extra samples from augmentation)
        min_len = min(len(orig_seg), len(aug_seg))
        orig_seg = orig_seg[:min_len]
        aug_seg  = aug_seg[:min_len]
        # Normalize volumes to avoid clipping (bring peak to 1.0)
        max_amp = max(orig_seg.abs().max().item(), aug_seg.abs().max().item())
        if max_amp > 1.0:
            orig_seg = orig_seg / max_amp
            aug_seg  = aug_seg / max_amp
        return orig_seg, aug_seg

# Dataset class for contrastive audio segments
class FMAContrastiveDataset(torch.utils.data.Dataset):
    def __init__(self, file_list, augmentor):
        self.files = file_list
        self.augmentor = augmentor
        # Predefine the mel spectrogram transform (for efficiency, reuse for all samples)
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=8000, n_fft=1024, hop_length=256, n_mels=256,
            f_min=300.0, f_max=4000.0, power=2.0
        )

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        # To allow more than one sample per track per epoch, if idx is beyond range, wrap around
        if idx >= len(self.files):
            idx = random.randrange(len(self.files))
        filepath = self.files[idx]
        # Load the audio file (mono waveform)
        try:
            # librosa can handle mp3 and various formats
            wav, sr = librosa.load(filepath, sr=None, mono=True)
            wav = torch.from_numpy(wav).float()
        except Exception as e:
            # If librosa fails, try torchaudio
            try:
                wav_t, sr = torchaudio.load(filepath)
                # torchaudio.load returns a tensor of shape [channels, length]
                wav = wav_t.mean(dim=0)  # convert to mono by averaging channels
            except Exception as e2:
                print(f"Could not load file {filepath}: {e2}")
                return None  # skip file if unreadable
        # Determine a random 1.0s segment for original and augmented pair
        track_duration = wav.shape[0] / sr
        window = 1.0  # 1 second window
        if track_duration <= window:
            orig_start = 0.0
        else:
            # random start time between 0.2s and (duration - 1.0s), to avoid starting too close to very beginning or end
            orig_start = random.uniform(0.2, max(0.2, track_duration - window))
        # Choose an augmented start time with small random offset (Â±0.2s)
        offset = random.uniform(-0.2, 0.2)
        aug_start = orig_start + offset
        if aug_start < 0:
            aug_start = 0.0
        if aug_start + window > track_duration:
            aug_start = max(0.0, track_duration - window)
        # Use the augmentor to get waveform segments
        global orig_audio_sr
        orig_audio_sr = sr  # set global sample rate for augmentor
        orig_seg, aug_seg = self.augmentor.process(wav, orig_start, aug_start)
        # Compute log-mel spectrograms for both segments
        orig_mel = self.mel_transform(orig_seg.unsqueeze(0))  # shape [1, 256, 32] for 1s segment
        aug_mel  = self.mel_transform(aug_seg.unsqueeze(0))
        # Convert power mel to log10 and normalize
        orig_mel_db = 10.0 * torch.log10(torch.clamp(orig_mel, min=1e-10))
        aug_mel_db  = 10.0 * torch.log10(torch.clamp(aug_mel,  min=1e-10))
        # Normalize so that the maximum value in each spectrogram is 0 dB
        orig_mel_db = orig_mel_db - orig_mel_db.max()
        aug_mel_db  = aug_mel_db  - aug_mel_db.max()
        # Clip the dynamic range to [-80, 0] dB
        orig_mel_db = torch.clamp(orig_mel_db, min=-80.0)
        aug_mel_db  = torch.clamp(aug_mel_db,  min=-80.0)
        return orig_mel_db, aug_mel_db

"""## Visualizing Audio Samples and Spectrograms"""

# Pick a random train track and visualize a 1-second segment and its augmentations
sample_track_path = random.choice(train_files)
print("Visualizing data for track file:", sample_track_path)

# Load the full track using torchaudio
wav, sr = torchaudio.load(sample_track_path)
wav = wav.mean(dim=0)  # mono
wav = wav.numpy()

# Consider the first 1.0 second of audio for visualization
segment_duration = 1.0  # seconds
segment_length = int(sr * segment_duration)
orig_segment = wav[:segment_length]
time_axis = np.linspace(0, segment_duration, num=len(orig_segment))

# Downsample to 8 kHz and compute log-mel spectrogram for the segment
resampled_seg = librosa.resample(orig_segment, orig_sr=sr, target_sr=8000)
mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=8000, n_fft=1024, hop_length=256, n_mels=256,
                                                    f_min=300.0, f_max=4000.0, power=2.0)
orig_mel = mel_transform(torch.from_numpy(resampled_seg).unsqueeze(0))
orig_mel_db = 10.0 * torch.log10(torch.clamp(orig_mel, min=1e-10))
orig_mel_db = orig_mel_db - orig_mel_db.max()
orig_mel_db = torch.clamp(orig_mel_db, min=-80.0).squeeze().numpy()

# Now apply the augmentor to get an augmented segment from the same track
augmentor = AudioAugmentor()  # use the same augmentor settings as dataset (white noise & synthetic IR)
orig_audio_sr = sr  # set global sample rate for augmentor

# Choose an offset 0.2s for augmentation for illustration (original start 0.0s, aug start 0.2s)
orig_start_time = 0.0
aug_start_time = 0.2 if (0.2 + 1.0 <= len(wav)/sr) else 0.0
orig_seg_tensor, aug_seg_tensor = augmentor.process(torch.from_numpy(wav), orig_start_time, aug_start_time)
# Compute spectrogram for augmented segment
aug_mel = mel_transform(aug_seg_tensor.unsqueeze(0))
aug_mel_db = 10.0 * torch.log10(torch.clamp(aug_mel, min=1e-10))
aug_mel_db = aug_mel_db - aug_mel_db.max()
aug_mel_db = torch.clamp(aug_mel_db, min=-80.0).squeeze().numpy()

# Plot the original and augmented waveforms (time-domain)
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(time_axis, orig_segment, color='steelblue')
plt.title("Original Audio Segment (first 1s)")
plt.xlabel("Time [s]")
plt.ylabel("Amplitude")

plt.subplot(1, 2, 2)
aug_segment_np = aug_seg_tensor.numpy()
time_axis_aug = np.linspace(0, len(aug_segment_np)/8000, num=len(aug_segment_np))
plt.plot(time_axis_aug, aug_segment_np, color='orange')
plt.title("Augmented Audio Segment (same track)")
plt.xlabel("Time [s]")
plt.ylabel("Amplitude")
plt.tight_layout()
plt.show()

# Plot the original and augmented spectrograms (log-mel)
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.imshow(orig_mel_db, aspect='auto', origin='lower', cmap='magma')
plt.title("Original Log-Mel Spectrogram")
plt.xlabel("Time frame")
plt.ylabel("Mel freq bin")

plt.subplot(1, 2, 2)
plt.imshow(aug_mel_db, aspect='auto', origin='lower', cmap='magma')
plt.title("Augmented Log-Mel Spectrogram")
plt.xlabel("Time frame")
plt.ylabel("Mel freq bin")
plt.tight_layout()
plt.show()

"""# Model Architecture: Neural Audio Fingerprinter"""

class AudioFingerprintModel(nn.Module):
    def __init__(self, fingerprint_dim=64):
        super().__init__()
        self.fingerprint_dim = fingerprint_dim
        # Convolutional encoder f(.)
        # Using architecture from the paper: 3 conv layers that reduce the time-frequency dimensions
        self.conv1 = nn.Conv2d(1, 16, kernel_size=(8, 4), stride=(4, 2), padding=(2, 1))   # output: [16, 64, 16]
        self.conv2 = nn.Conv2d(16, 32, kernel_size=(8, 4), stride=(4, 2), padding=(2, 1))  # output: [32, 16, 8]
        self.conv3 = nn.Conv2d(32, 64, kernel_size=(8, 4), stride=(4, 2), padding=(2, 1))  # output: [64, 4, 4]
        # Normalization layers after each conv (GroupNorm with 1 group = LayerNorm effect per channel)
        self.norm1 = nn.GroupNorm(1, 16)
        self.norm2 = nn.GroupNorm(1, 32)
        self.norm3 = nn.GroupNorm(1, 64)
        # Projection head g(.): split 1024-dim flattened encoder output into d chunks, each fed to a small MLP
        # Each head: Linear(1024/d -> 32) + ELU, then Linear(32 -> 1). Use ModuleList to create d heads.
        self.head_linears1 = nn.ModuleList([nn.Linear(1024 // fingerprint_dim, 32) for _ in range(fingerprint_dim)])
        self.head_linears2 = nn.ModuleList([nn.Linear(32, 1) for _ in range(fingerprint_dim)])

    def forward(self, x):
        # x shape: (B, 1, 256, 32) where B is batch size
        # Encoder forward pass
        x = F.relu(self.norm1(self.conv1(x)))   # -> (B, 16, 64, 16)
        x = F.relu(self.norm2(self.conv2(x)))   # -> (B, 32, 16, 8)
        x = F.relu(self.norm3(self.conv3(x)))   # -> (B, 64, 4, 4)
        # Flatten spatial dimensions (64 * 4 * 4 = 1024 features per sample)
        x = x.view(x.size(0), -1)               # -> (B, 1024)
        # Projection head: split into d chunks and process each with its own head
        chunks = torch.split(x, 1024 // self.fingerprint_dim, dim=1)  # tuple of length d, each (B, 1024/d)
        out_components = []
        for i in range(self.fingerprint_dim):
            h = F.elu(self.head_linears1[i](chunks[i]))  # (B, 32)
            o = self.head_linears2[i](h)                 # (B, 1)
            out_components.append(o)
        z = torch.cat(out_components, dim=1)  # concat all head outputs -> (B, d)
        # L2-normalize the fingerprint embeddings
        z = F.normalize(z, p=2, dim=1)
        return z

# Instantiate the model and move to device
model = AudioFingerprintModel(fingerprint_dim=64).to(device)
print(model)

"""# Contrastive Training Setup"""

# Training hyperparameters
temperature = 0.05
learning_rate = 1e-4
max_epochs = 200
max_train_samples = None  # Use all data
batch_size = 128          # If GPU memory allows

# Optimizer: Adam (suitable for smaller batch sizes; LAMB is beneficial for very large batches&#8203;:contentReference[oaicite:10]{index=10})
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
# Learning rate scheduler: Cosine decay (no restarts, no warmup)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs * (len(train_df)//64))
# Mixed precision scaler
scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None

# DataLoader for training
augmentor = AudioAugmentor(noise_files=None)  # using white noise for augmentation
train_dataset = FMAContrastiveDataset(train_files, augmentor)
# Optionally use a subset of the training data for faster epochs (for demo purposes)
max_train_samples = 1000  # set to None to use full training set
if max_train_samples is not None:
    train_dataset = torch.utils.data.Subset(train_dataset, range(min(max_train_samples, len(train_dataset))))
# DataLoader with multiple workers for speed (adjust num_workers as appropriate for environment)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True,
                                           num_workers=2, pin_memory=True, drop_last=True)

# Set model to training mode
model.train()

apply_spec_augment = False  # Toggle spectrogram masking augmentation

loss_history = []
for epoch in range(1, max_epochs+1):
    total_loss = 0.0
    for batch_idx, batch in enumerate(train_loader, start=1):
        if batch is None or batch[0] is None:
            # Skip if any file failed to load in this batch
            continue
        orig_batch, aug_batch = batch
        # Move to GPU if available
        orig_batch = orig_batch.to(device)
        aug_batch  = aug_batch.to(device)
        batch_size = orig_batch.size(0)
        # Optional SpecAugment: mask time/freq bands
        if apply_spec_augment:
            # Random time-frequency masking on the batch
            # Time mask
            t_max = orig_batch.shape[-1]
            t_len = random.randint(int(0.1*t_max), int(0.5*t_max))
            t_start = random.randint(0, t_max - t_len) if t_max > t_len else 0
            orig_batch[:, :, :, t_start:t_start+t_len] = 0.0
            aug_batch[:, :, :, t_start:t_start+t_len]  = 0.0
            # Frequency mask
            f_max = orig_batch.shape[-2]
            f_len = random.randint(int(0.1*f_max), int(0.5*f_max))
            f_start = random.randint(0, f_max - f_len) if f_max > f_len else 0
            orig_batch[:, :, f_start:f_start+f_len, :] = 0.0
            aug_batch[:, :, f_start:f_start+f_len, :]  = 0.0

        # Forward pass with mixed precision
        with torch.cuda.amp.autocast(enabled=(device=='cuda')):
            combined = torch.cat([orig_batch, aug_batch], dim=0)  # shape (2B, 1, 256, 32)
            embeddings = model(combined)                           # shape (2B, d)
            orig_emb = embeddings[:batch_size]  # first B are originals
            aug_emb  = embeddings[batch_size:]  # last B are augmented
            # Compute similarity matrix (orig vs aug) -> shape (B, B)
            sim_matrix = torch.matmul(orig_emb, aug_emb.T)  # inner products
            targets = torch.arange(batch_size, device=device)  # 0..B-1
            loss = F.cross_entropy(sim_matrix / temperature, targets)
        # Backpropagation
        if scaler:
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            optimizer.step()
        optimizer.zero_grad()
        scheduler.step()

        total_loss += loss.item()
        if batch_idx % 50 == 0:
            print(f"Epoch {epoch} Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}")
    avg_loss = total_loss / len(train_loader)
    loss_history.append(avg_loss)
    print(f"*** Epoch {epoch} complete â Average Loss: {avg_loss:.4f} ***")

# Plot training loss history
plt.figure(figsize=(6,4))
plt.plot(range(1, len(loss_history)+1), loss_history, marker='o')
plt.title("Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Avg. Contrastive Loss")
plt.grid(True)
plt.show()

"""# Building the Fingerprint Database with FAISS"""

import faiss

# Ensure model is in eval mode and on CPU for inference (we can use CPU for indexing to save GPU memory)
model.eval()
model.cpu()

# Build the fingerprint index (Flat inner-product index since embeddings are normalized)
d = 64  # dimension of fingerprint vectors
index = faiss.IndexFlatIP(d)  # Inner-product because our embeddings are L2-normalized (cosine similarity)

# Lists to hold track IDs and their fingerprint vectors
track_ids = list(test_df.index)
fingerprints = []

print("Computing fingerprints for each test track...")
for track_id, file_path in zip(track_ids, test_files):
    # Load audio
    try:
        wav, sr = torchaudio.load(file_path)
        wav = wav.mean(dim=0)  # mono
    except Exception as e:
        # If torchaudio fails, try librosa
        try:
            audio_np, sr = librosa.load(file_path, sr=None, mono=True)
            wav = torch.from_numpy(audio_np)
        except Exception as e2:
            print(f"Skipping track {track_id}, error loading audio.")
            continue
    # Choose a reference segment (we'll just take the first 1s of the track for simplicity)
    if wav.shape[0] < sr:  # if track shorter than 1s, skip
        print(f"Track {track_id} is shorter than 1 second, skipping.")
        continue
    ref_start = 0.0  # start at beginning
    orig_audio_sr = sr
    # Use augmentor to get a clean segment (the augmentor's process will still resample it to 8k and return orig_seg without added noise)
    augmentor = AudioAugmentor(noise_files=None)
    orig_seg, _ = augmentor.process(wav, orig_start=ref_start, aug_start=ref_start)  # using same start for orig and aug to get orig_seg
    # Compute embedding for this segment
    mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=8000, n_fft=1024, hop_length=256, n_mels=256,
                                                   f_min=300.0, f_max=4000.0, power=2.0)(orig_seg.unsqueeze(0))
    mel_db = 10.0 * torch.log10(torch.clamp(mel_spec, min=1e-10))
    mel_db = mel_db - mel_db.max()
    mel_db = torch.clamp(mel_db, min=-80.0)
    mel_db = mel_db.unsqueeze(0)  # add batch dim
    with torch.no_grad():
        emb = model(mel_db).squeeze(0).numpy().astype('float32')
    fingerprints.append(emb)

# Add all fingerprints to the FAISS index
fingerprints = np.stack(fingerprints)
index.add(fingerprints)
print(f"FAISS index built with {index.ntotal} vectors.")

# Evaluate retrieval on test tracks
K = 5  # we'll consider Recall@5 and Precision@5
top1_count = 0
recall_K_count = 0
precision_at_K_sum = 0.0

print("Searching for each test query in the fingerprint database...")
for i, (track_id, file_path) in enumerate(zip(track_ids, test_files)):
    # Load audio (as before)
    try:
        wav, sr = torchaudio.load(file_path)
        wav = wav.mean(dim=0)
    except:
        try:
            audio_np, sr = librosa.load(file_path, sr=None, mono=True)
            wav = torch.from_numpy(audio_np)
        except:
            continue
    if wav.shape[0] < sr:
        continue  # skip tracks too short
    # Generate a query segment that starts at 1s (different from reference which was 0s), or 0.5s offset if track is short
    query_start = 1.0 if (1.0 + 1.0 <= wav.shape[0]/sr) else 0.0
    # Use augmentor to get an augmented segment for query (with time offset + noise/IR)
    orig_audio_sr = sr
    augmentor = AudioAugmentor(noise_files=None)
    _, query_seg = augmentor.process(wav, orig_start=query_start, aug_start=min(query_start + 0.1, max(0.0, (wav.shape[0]/sr) - 1.0)))
    # Compute fingerprint for query segment
    mel_spec_q = torchaudio.transforms.MelSpectrogram(sample_rate=8000, n_fft=1024, hop_length=256, n_mels=256,
                                                     f_min=300.0, f_max=4000.0, power=2.0)(query_seg.unsqueeze(0))
    mel_db_q = 10.0 * torch.log10(torch.clamp(mel_spec_q, min=1e-10))
    mel_db_q = mel_db_q - mel_db_q.max()
    mel_db_q = torch.clamp(mel_db_q, min=-80.0)
    mel_db_q = mel_db_q.unsqueeze(0)
    with torch.no_grad():
        query_emb = model(mel_db_q).numpy().astype('float32')
    # Search the index for the top K nearest neighbors to the query
    D, I = index.search(query_emb, K)  # I: indices of nearest neighbors, D: distances (cosine similarity since normalized)
    retrieved_indices = I[0]  # indices of results for this query
    # Check if the correct track is in the results
    # We know the index of the correct track's reference in the index is "i" (since we added in order).
    correct_index = i
    if correct_index < index.ntotal:
        # Top-1 accuracy
        if retrieved_indices[0] == correct_index:
            top1_count += 1
        # Recall@K
        if correct_index in retrieved_indices:
            recall_K_count += 1
            # Precision@K (only one relevant item per query)
            precision_at_K_sum += 1.0 / K
    else:
        # If track was skipped in indexing for some reason (e.g., too short), ignore it in metrics
        pass

total_queries = len(track_ids)
top1_acc = top1_count / total_queries * 100
recall_K = recall_K_count / total_queries * 100
precision_K = precision_at_K_sum / total_queries * 100

print(f"Top-1 Accuracy: {top1_acc:.2f}%")
print(f"Recall@{K}: {recall_K:.2f}%")
print(f"Precision@{K}: {precision_K:.2f}%")

"""# Embedding Similarity Heatmap"""

# Select a small subset of test tracks to visualize embedding similarities
subset_tracks = track_ids[:5]  # first 5 test tracks
embeddings = []
labels = []
for tid in subset_tracks:
    file_path = get_audio_path(tid)
    # Load audio
    try:
        wav, sr = torchaudio.load(file_path)
        wav = wav.mean(dim=0)
    except:
        audio_np, sr = librosa.load(file_path, sr=None, mono=True)
        wav = torch.from_numpy(audio_np)
    # Take multiple segments from each track
    for seg_idx in range(3):  # take 3 random segments per track
        if wav.shape[0] < sr:
            continue
        start_time = random.uniform(0, max(0, wav.shape[0]/sr - 1.0))
        orig_audio_sr = sr
        augmentor = AudioAugmentor(noise_files=None)
        # We won't add extra noise here; just get a clean segment at different times (use same start for orig and aug to get orig_seg)
        orig_seg, _ = augmentor.process(wav, orig_start=start_time, aug_start=start_time)
        # Compute embedding
        mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=8000, n_fft=1024, hop_length=256, n_mels=256,
                                                       f_min=300.0, f_max=4000.0, power=2.0)(orig_seg.unsqueeze(0))
        mel_db = 10.0 * torch.log10(torch.clamp(mel_spec, min=1e-10))
        mel_db = mel_db - mel_db.max()
        mel_db = torch.clamp(mel_db, min=-80.0).unsqueeze(0)
        with torch.no_grad():
            emb = model(mel_db).numpy().flatten()
        embeddings.append(emb)
        labels.append(tid)
# Compute cosine similarity matrix for these embeddings
embeddings = np.vstack(embeddings)
# They are L2-normalized, so cosine similarity is just dot product
sim_matrix = np.dot(embeddings, embeddings.T)
# Plot the similarity matrix
plt.figure(figsize=(6,5))
plt.imshow(sim_matrix, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1)
plt.colorbar(label='Cosine similarity')
plt.title("Embedding Similarity Heatmap (small subset of tracks)")
plt.xlabel("Segment index")
plt.ylabel("Segment index")
plt.show()